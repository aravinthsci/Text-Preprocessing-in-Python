{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Preprocessing in Python.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "DRw76WwxZ4uw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Basic steps of text preprocessing, which are needed for transferring text from human language to machine-readable format for further processing**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "LnXN7xOwZTK0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Convert text to lowercase**"
      ]
    },
    {
      "metadata": {
        "id": "w4TrS5dpZWzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6c2dfda1-b230-49db-a406-a02cb0be120a"
      },
      "cell_type": "code",
      "source": [
        "input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n",
        "input_str = input_str.lower()\n",
        "print(input_str)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AXYqCp9GaWjg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Remove numbers**\n",
        "\n",
        "Remove numbers if they are not relevant to your analyses. Usually, regular expressions are used to remove numbers."
      ]
    },
    {
      "metadata": {
        "id": "0aS_Mbu_aYOV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "df603411-c8e4-41af-dfda-81b1ce00b096"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "input_str = 'Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.'\n",
        "result = re.sub(r'\\d+', '', input_str)\n",
        "print(result)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_s_lYWaHbXhT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Remove punctuation**\n",
        "\n",
        "The following code removes this set of symbols [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]:"
      ]
    },
    {
      "metadata": {
        "id": "xYPl5eHcbWi5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eff9b9af-60c7-4689-c352-9f29e525a18e"
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "input_str = \"This &is [an] example? {of} string. with.? punctuation!!!!\" # Sample string\n",
        "result = re.sub(r'[^\\w\\s]', '', input_str)\n",
        "#result = input_str.translate(str.maketrans(\"\"), string.punctuation)\n",
        "print(result)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is an example of string with punctuation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RVohZbp7cwrV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Remove whitespaces**\n",
        "\n",
        "To remove leading and ending spaces, you can use the strip() function:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "YV52AxAPbwow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5069c5a5-a0c7-467b-bf30-497a15949bd2"
      },
      "cell_type": "code",
      "source": [
        "input_str = \" \\t a string example\\t \"\n",
        "input_str = input_str.strip()\n",
        "input_str"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a string example'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "MnRhyJOceaBl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Remove stop words**\n",
        "\n",
        "“Stop words” are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts. It is possible to remove stop words using Natural Language Toolkit (NLTK), a suite of libraries and programs for symbolic and statistical natural language processing."
      ]
    },
    {
      "metadata": {
        "id": "-UECgFoce055",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "auTjAXJdc4sM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9911f23e-7c86-4f04-9517-b9fda4d5687f"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "  \n",
        "example_sent = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "  \n",
        "stop_words = set(stopwords.words('english')) \n",
        "  \n",
        "word_tokens = word_tokenize(example_sent) \n",
        "  \n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "  \n",
        "filtered_sentence = [] \n",
        "  \n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w) \n",
        "  \n",
        "print(word_tokens) \n",
        "print(filtered_sentence) \n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n",
            "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wmYe4flRiyn9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Stemming**\n",
        "\n",
        "Stemming is a process of reducing words to their word stem, base or root form (for example, books — book, looked — look). The main two algorithms are Porter stemming algorithm (removes common morphological and inflexional endings from words ) and Lancaster stemming algorithm (a more aggressive stemming algorithm)."
      ]
    },
    {
      "metadata": {
        "id": "D0H4U6RCf9A8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "2d38a1fd-eba5-4867-973d-c219033e64f3"
      },
      "cell_type": "code",
      "source": [
        " \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer= PorterStemmer()\n",
        "input_str=\"There are several types of stemming algorithms.\"\n",
        "input_str=word_tokenize(input_str)\n",
        "for word in input_str:\n",
        "    print(stemmer.stem(word))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there\n",
            "are\n",
            "sever\n",
            "type\n",
            "of\n",
            "stem\n",
            "algorithm\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pe26HvWBjNuY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**\n",
        "\n",
        "The aim of lemmatization, like stemming, is to reduce inflectional forms to a common base form. As opposed to stemming, lemmatization does not simply chop off inflections. Instead it uses lexical knowledge bases to get the correct base forms of words."
      ]
    },
    {
      "metadata": {
        "id": "4WjIptPcjB1Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "d9c0fecd-0884-4496-c101-8d65258f2af4"
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "input_str=\"been had done languages cities mice\"\n",
        "input_str=word_tokenize(input_str)\n",
        "for word in input_str:\n",
        "    print(lemmatizer.lemmatize(word))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "been\n",
            "had\n",
            "done\n",
            "language\n",
            "city\n",
            "mouse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZILnVSO1jjGn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Part of speech tagging (POS)**\n",
        "\n",
        "Part-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context. "
      ]
    },
    {
      "metadata": {
        "id": "q6Eti3CwjY1Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "55f6fe78-4011-4367-e530-44a644301749"
      },
      "cell_type": "code",
      "source": [
        " \n",
        "input_str=\"Parts of speech examples: an article, to write, interesting, easily, and, of\"\n",
        "from textblob import TextBlob\n",
        "result = TextBlob(input_str)\n",
        "print(result.tags)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), ('an', 'DT'), ('article', 'NN'), ('to', 'TO'), ('write', 'VB'), ('interesting', 'VBG'), ('easily', 'RB'), ('and', 'CC'), ('of', 'IN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aUPsZXUOj8R0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Chunking (shallow parsing)**\n",
        "\n",
        "Chunking is a natural language process that identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.)"
      ]
    },
    {
      "metadata": {
        "id": "mF0dUyizjpSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "97eaf568-a8cb-4743-8cb9-4ceb5c60833d"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "input_str=\"A black television and a white stove were bought for the new apartment of John.\"\n",
        "from textblob import TextBlob\n",
        "result = TextBlob(input_str)\n",
        "print(result.tags)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('A', 'DT'), ('black', 'JJ'), ('television', 'NN'), ('and', 'CC'), ('a', 'DT'), ('white', 'JJ'), ('stove', 'NN'), ('were', 'VBD'), ('bought', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('apartment', 'NN'), ('of', 'IN'), ('John', 'NNP')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SD11KPrWkLGs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "2e1b0e41-7e02-4ab9-fda3-6158a31c01ee"
      },
      "cell_type": "code",
      "source": [
        "grammar = ('''\n",
        "    NP: {<DT>?<JJ>*<NN>} # NP\n",
        "    ''')\n",
        "rp = nltk.RegexpParser(grammar)\n",
        "result = rp.parse(result.tags)\n",
        "print(result)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP A/DT black/JJ television/NN)\n",
            "  and/CC\n",
            "  (NP a/DT white/JJ stove/NN)\n",
            "  were/VBD\n",
            "  bought/VBN\n",
            "  for/IN\n",
            "  (NP the/DT new/JJ apartment/NN)\n",
            "  of/IN\n",
            "  John/NNP)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jvgYZ6ntm3QG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Named entity recognition**\n",
        "\n",
        "Named-entity recognition (NER) aims to find named entities in text and classify them into pre-defined categories (names of persons, locations, organizations, times, etc.)."
      ]
    },
    {
      "metadata": {
        "id": "pbkkSraYlSyP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "8a51522d-e580-4162-87c4-141e616440c5"
      },
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "input_str = \"Bill works for Apple so he went to Boston for a conference.\"\n",
        "print(ne_chunk(pos_tag(word_tokenize(input_str))))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (PERSON Bill/NNP)\n",
            "  works/VBZ\n",
            "  for/IN\n",
            "  Apple/NNP\n",
            "  so/IN\n",
            "  he/PRP\n",
            "  went/VBD\n",
            "  to/TO\n",
            "  (GPE Boston/NNP)\n",
            "  for/IN\n",
            "  a/DT\n",
            "  conference/NN\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uCaPMtfxoFFO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}